{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e34883e-1e48-4783-a518-4849ea937fc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### In this notebook, creating Delta Live Tables (DLT) and views to process and transform data. Here is a brief explanation of what each cell is doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a18bf7c1-db46-4260-80c3-18c5af24c9c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary functions and libraries\n",
    "from pyspark.sql.functions import col, split, mean\n",
    "import dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "219e7d0e-30e0-49b8-a225-950d4dd3167c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a Delta Live Table (DLT) stream table\n",
    "@dlt.table\n",
    "def stream_table1():\n",
    "    # Read streaming data from the specified source table\n",
    "    df = spark.readStream.table(\"c1.s1.external_customertable\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdd025f9-6336-4a9e-bd72-74ac4291b914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a Delta Live Table (DLT) view\n",
    "@dlt.view\n",
    "def view_table():\n",
    "    # Read data from the specified source table\n",
    "    df = spark.read.table(\"LIVE.c1.s1.external_customertable\")\n",
    "    \n",
    "    # Group by 'age' and calculate the mean of 'age', renaming it to 'avg_age'\n",
    "    df = df.groupBy(\"age\").agg(mean('age').alias(\"avg_age\"))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09780007-7da3-4ee8-aab6-1ab37ddbce13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table\n",
    "def materialized_customer_view():\n",
    "    # Read data from the Delta Live Table stream_table1\n",
    "    df = spark.read.table(\"LIVE.stream_table1\")\n",
    "    \n",
    "    # Extract the part of the email before '@' and create a new column 'Last_name'\n",
    "    df = df.withColumn(\"Last_name\", split(col(\"email\"), '@')[0])\n",
    "    \n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DLT-1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
